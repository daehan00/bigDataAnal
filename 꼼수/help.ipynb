{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-15T12:25:13.287231Z",
     "start_time": "2025-06-15T12:25:13.016189Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "\n",
    "print(help(pd.DataFrame.loc))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on property:\n",
      "\n",
      "    Access a group of rows and columns by label(s) or a boolean array.\n",
      "    \n",
      "    ``.loc[]`` is primarily label based, but may also be used with a\n",
      "    boolean array.\n",
      "    \n",
      "    Allowed inputs are:\n",
      "    \n",
      "    - A single label, e.g. ``5`` or ``'a'``, (note that ``5`` is\n",
      "      interpreted as a *label* of the index, and **never** as an\n",
      "      integer position along the index).\n",
      "    - A list or array of labels, e.g. ``['a', 'b', 'c']``.\n",
      "    - A slice object with labels, e.g. ``'a':'f'``.\n",
      "    \n",
      "      .. warning:: Note that contrary to usual python slices, **both** the\n",
      "          start and the stop are included\n",
      "    \n",
      "    - A boolean array of the same length as the axis being sliced,\n",
      "      e.g. ``[True, False, True]``.\n",
      "    - An alignable boolean Series. The index of the key will be aligned before\n",
      "      masking.\n",
      "    - An alignable Index. The Index of the returned selection will be the input.\n",
      "    - A ``callable`` function with one argument (the calling Series or\n",
      "      DataFrame) and that returns valid output for indexing (one of the above)\n",
      "    \n",
      "    See more at :ref:`Selection by Label <indexing.label>`.\n",
      "    \n",
      "    Raises\n",
      "    ------\n",
      "    KeyError\n",
      "        If any items are not found.\n",
      "    IndexingError\n",
      "        If an indexed key is passed and its index is unalignable to the frame index.\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    DataFrame.at : Access a single value for a row/column label pair.\n",
      "    DataFrame.iloc : Access group of rows and columns by integer position(s).\n",
      "    DataFrame.xs : Returns a cross-section (row(s) or column(s)) from the\n",
      "                   Series/DataFrame.\n",
      "    Series.loc : Access group of values using labels.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    **Getting values**\n",
      "    \n",
      "    >>> df = pd.DataFrame([[1, 2], [4, 5], [7, 8]],\n",
      "    ...                   index=['cobra', 'viper', 'sidewinder'],\n",
      "    ...                   columns=['max_speed', 'shield'])\n",
      "    >>> df\n",
      "                max_speed  shield\n",
      "    cobra               1       2\n",
      "    viper               4       5\n",
      "    sidewinder          7       8\n",
      "    \n",
      "    Single label. Note this returns the row as a Series.\n",
      "    \n",
      "    >>> df.loc['viper']\n",
      "    max_speed    4\n",
      "    shield       5\n",
      "    Name: viper, dtype: int64\n",
      "    \n",
      "    List of labels. Note using ``[[]]`` returns a DataFrame.\n",
      "    \n",
      "    >>> df.loc[['viper', 'sidewinder']]\n",
      "                max_speed  shield\n",
      "    viper               4       5\n",
      "    sidewinder          7       8\n",
      "    \n",
      "    Single label for row and column\n",
      "    \n",
      "    >>> df.loc['cobra', 'shield']\n",
      "    2\n",
      "    \n",
      "    Slice with labels for row and single label for column. As mentioned\n",
      "    above, note that both the start and stop of the slice are included.\n",
      "    \n",
      "    >>> df.loc['cobra':'viper', 'max_speed']\n",
      "    cobra    1\n",
      "    viper    4\n",
      "    Name: max_speed, dtype: int64\n",
      "    \n",
      "    Boolean list with the same length as the row axis\n",
      "    \n",
      "    >>> df.loc[[False, False, True]]\n",
      "                max_speed  shield\n",
      "    sidewinder          7       8\n",
      "    \n",
      "    Alignable boolean Series:\n",
      "    \n",
      "    >>> df.loc[pd.Series([False, True, False],\n",
      "    ...                  index=['viper', 'sidewinder', 'cobra'])]\n",
      "                         max_speed  shield\n",
      "    sidewinder          7       8\n",
      "    \n",
      "    Index (same behavior as ``df.reindex``)\n",
      "    \n",
      "    >>> df.loc[pd.Index([\"cobra\", \"viper\"], name=\"foo\")]\n",
      "           max_speed  shield\n",
      "    foo\n",
      "    cobra          1       2\n",
      "    viper          4       5\n",
      "    \n",
      "    Conditional that returns a boolean Series\n",
      "    \n",
      "    >>> df.loc[df['shield'] > 6]\n",
      "                max_speed  shield\n",
      "    sidewinder          7       8\n",
      "    \n",
      "    Conditional that returns a boolean Series with column labels specified\n",
      "    \n",
      "    >>> df.loc[df['shield'] > 6, ['max_speed']]\n",
      "                max_speed\n",
      "    sidewinder          7\n",
      "    \n",
      "    Multiple conditional using ``&`` that returns a boolean Series\n",
      "    \n",
      "    >>> df.loc[(df['max_speed'] > 1) & (df['shield'] < 8)]\n",
      "                max_speed  shield\n",
      "    viper          4       5\n",
      "    \n",
      "    Multiple conditional using ``|`` that returns a boolean Series\n",
      "    \n",
      "    >>> df.loc[(df['max_speed'] > 4) | (df['shield'] < 5)]\n",
      "                max_speed  shield\n",
      "    cobra               1       2\n",
      "    sidewinder          7       8\n",
      "    \n",
      "    Please ensure that each condition is wrapped in parentheses ``()``.\n",
      "    See the :ref:`user guide<indexing.boolean>`\n",
      "    for more details and explanations of Boolean indexing.\n",
      "    \n",
      "    .. note::\n",
      "        If you find yourself using 3 or more conditionals in ``.loc[]``,\n",
      "        consider using :ref:`advanced indexing<advanced.advanced_hierarchical>`.\n",
      "    \n",
      "        See below for using ``.loc[]`` on MultiIndex DataFrames.\n",
      "    \n",
      "    Callable that returns a boolean Series\n",
      "    \n",
      "    >>> df.loc[lambda df: df['shield'] == 8]\n",
      "                max_speed  shield\n",
      "    sidewinder          7       8\n",
      "    \n",
      "    **Setting values**\n",
      "    \n",
      "    Set value for all items matching the list of labels\n",
      "    \n",
      "    >>> df.loc[['viper', 'sidewinder'], ['shield']] = 50\n",
      "    >>> df\n",
      "                max_speed  shield\n",
      "    cobra               1       2\n",
      "    viper               4      50\n",
      "    sidewinder          7      50\n",
      "    \n",
      "    Set value for an entire row\n",
      "    \n",
      "    >>> df.loc['cobra'] = 10\n",
      "    >>> df\n",
      "                max_speed  shield\n",
      "    cobra              10      10\n",
      "    viper               4      50\n",
      "    sidewinder          7      50\n",
      "    \n",
      "    Set value for an entire column\n",
      "    \n",
      "    >>> df.loc[:, 'max_speed'] = 30\n",
      "    >>> df\n",
      "                max_speed  shield\n",
      "    cobra              30      10\n",
      "    viper              30      50\n",
      "    sidewinder         30      50\n",
      "    \n",
      "    Set value for rows matching callable condition\n",
      "    \n",
      "    >>> df.loc[df['shield'] > 35] = 0\n",
      "    >>> df\n",
      "                max_speed  shield\n",
      "    cobra              30      10\n",
      "    viper               0       0\n",
      "    sidewinder          0       0\n",
      "    \n",
      "    Add value matching location\n",
      "    \n",
      "    >>> df.loc[\"viper\", \"shield\"] += 5\n",
      "    >>> df\n",
      "                max_speed  shield\n",
      "    cobra              30      10\n",
      "    viper               0       5\n",
      "    sidewinder          0       0\n",
      "    \n",
      "    Setting using a ``Series`` or a ``DataFrame`` sets the values matching the\n",
      "    index labels, not the index positions.\n",
      "    \n",
      "    >>> shuffled_df = df.loc[[\"viper\", \"cobra\", \"sidewinder\"]]\n",
      "    >>> df.loc[:] += shuffled_df\n",
      "    >>> df\n",
      "                max_speed  shield\n",
      "    cobra              60      20\n",
      "    viper               0      10\n",
      "    sidewinder          0       0\n",
      "    \n",
      "    **Getting values on a DataFrame with an index that has integer labels**\n",
      "    \n",
      "    Another example using integers for the index\n",
      "    \n",
      "    >>> df = pd.DataFrame([[1, 2], [4, 5], [7, 8]],\n",
      "    ...                   index=[7, 8, 9], columns=['max_speed', 'shield'])\n",
      "    >>> df\n",
      "       max_speed  shield\n",
      "    7          1       2\n",
      "    8          4       5\n",
      "    9          7       8\n",
      "    \n",
      "    Slice with integer labels for rows. As mentioned above, note that both\n",
      "    the start and stop of the slice are included.\n",
      "    \n",
      "    >>> df.loc[7:9]\n",
      "       max_speed  shield\n",
      "    7          1       2\n",
      "    8          4       5\n",
      "    9          7       8\n",
      "    \n",
      "    **Getting values with a MultiIndex**\n",
      "    \n",
      "    A number of examples using a DataFrame with a MultiIndex\n",
      "    \n",
      "    >>> tuples = [\n",
      "    ...     ('cobra', 'mark i'), ('cobra', 'mark ii'),\n",
      "    ...     ('sidewinder', 'mark i'), ('sidewinder', 'mark ii'),\n",
      "    ...     ('viper', 'mark ii'), ('viper', 'mark iii')\n",
      "    ... ]\n",
      "    >>> index = pd.MultiIndex.from_tuples(tuples)\n",
      "    >>> values = [[12, 2], [0, 4], [10, 20],\n",
      "    ...           [1, 4], [7, 1], [16, 36]]\n",
      "    >>> df = pd.DataFrame(values, columns=['max_speed', 'shield'], index=index)\n",
      "    >>> df\n",
      "                         max_speed  shield\n",
      "    cobra      mark i           12       2\n",
      "               mark ii           0       4\n",
      "    sidewinder mark i           10      20\n",
      "               mark ii           1       4\n",
      "    viper      mark ii           7       1\n",
      "               mark iii         16      36\n",
      "    \n",
      "    Single label. Note this returns a DataFrame with a single index.\n",
      "    \n",
      "    >>> df.loc['cobra']\n",
      "             max_speed  shield\n",
      "    mark i          12       2\n",
      "    mark ii          0       4\n",
      "    \n",
      "    Single index tuple. Note this returns a Series.\n",
      "    \n",
      "    >>> df.loc[('cobra', 'mark ii')]\n",
      "    max_speed    0\n",
      "    shield       4\n",
      "    Name: (cobra, mark ii), dtype: int64\n",
      "    \n",
      "    Single label for row and column. Similar to passing in a tuple, this\n",
      "    returns a Series.\n",
      "    \n",
      "    >>> df.loc['cobra', 'mark i']\n",
      "    max_speed    12\n",
      "    shield        2\n",
      "    Name: (cobra, mark i), dtype: int64\n",
      "    \n",
      "    Single tuple. Note using ``[[]]`` returns a DataFrame.\n",
      "    \n",
      "    >>> df.loc[[('cobra', 'mark ii')]]\n",
      "                   max_speed  shield\n",
      "    cobra mark ii          0       4\n",
      "    \n",
      "    Single tuple for the index with a single label for the column\n",
      "    \n",
      "    >>> df.loc[('cobra', 'mark i'), 'shield']\n",
      "    2\n",
      "    \n",
      "    Slice from index tuple to single label\n",
      "    \n",
      "    >>> df.loc[('cobra', 'mark i'):'viper']\n",
      "                         max_speed  shield\n",
      "    cobra      mark i           12       2\n",
      "               mark ii           0       4\n",
      "    sidewinder mark i           10      20\n",
      "               mark ii           1       4\n",
      "    viper      mark ii           7       1\n",
      "               mark iii         16      36\n",
      "    \n",
      "    Slice from index tuple to index tuple\n",
      "    \n",
      "    >>> df.loc[('cobra', 'mark i'):('viper', 'mark ii')]\n",
      "                        max_speed  shield\n",
      "    cobra      mark i          12       2\n",
      "               mark ii          0       4\n",
      "    sidewinder mark i          10      20\n",
      "               mark ii          1       4\n",
      "    viper      mark ii          7       1\n",
      "    \n",
      "    Please see the :ref:`user guide<advanced.advanced_hierarchical>`\n",
      "    for more details and explanations of advanced indexing.\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T12:34:58.769565Z",
     "start_time": "2025-06-15T12:34:58.766374Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sklearn.ensemble\n",
    "print(sklearn.ensemble.__all__)"
   ],
   "id": "db56b8e449352a88",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BaseEnsemble', 'RandomForestClassifier', 'RandomForestRegressor', 'RandomTreesEmbedding', 'ExtraTreesClassifier', 'ExtraTreesRegressor', 'BaggingClassifier', 'BaggingRegressor', 'IsolationForest', 'GradientBoostingClassifier', 'GradientBoostingRegressor', 'AdaBoostClassifier', 'AdaBoostRegressor', 'VotingClassifier', 'VotingRegressor', 'StackingClassifier', 'StackingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor']\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T12:35:42.335278Z",
     "start_time": "2025-06-15T12:35:42.331153Z"
    }
   },
   "cell_type": "code",
   "source": "print(help(sklearn.ensemble.AdaBoostRegressor))",
   "id": "5cebc53d36287bfc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class AdaBoostRegressor in module sklearn.ensemble._weight_boosting:\n",
      "\n",
      "class AdaBoostRegressor(sklearn.utils._metadata_requests._RoutingNotSupportedMixin, sklearn.base.RegressorMixin, BaseWeightBoosting)\n",
      " |  AdaBoostRegressor(estimator=None, *, n_estimators=50, learning_rate=1.0, loss='linear', random_state=None)\n",
      " |  \n",
      " |  An AdaBoost regressor.\n",
      " |  \n",
      " |  An AdaBoost [1] regressor is a meta-estimator that begins by fitting a\n",
      " |  regressor on the original dataset and then fits additional copies of the\n",
      " |  regressor on the same dataset but where the weights of instances are\n",
      " |  adjusted according to the error of the current prediction. As such,\n",
      " |  subsequent regressors focus more on difficult cases.\n",
      " |  \n",
      " |  This class implements the algorithm known as AdaBoost.R2 [2].\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <adaboost>`.\n",
      " |  \n",
      " |  .. versionadded:: 0.14\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  estimator : object, default=None\n",
      " |      The base estimator from which the boosted ensemble is built.\n",
      " |      If ``None``, then the base estimator is\n",
      " |      :class:`~sklearn.tree.DecisionTreeRegressor` initialized with\n",
      " |      `max_depth=3`.\n",
      " |  \n",
      " |      .. versionadded:: 1.2\n",
      " |         `base_estimator` was renamed to `estimator`.\n",
      " |  \n",
      " |  n_estimators : int, default=50\n",
      " |      The maximum number of estimators at which boosting is terminated.\n",
      " |      In case of perfect fit, the learning procedure is stopped early.\n",
      " |      Values must be in the range `[1, inf)`.\n",
      " |  \n",
      " |  learning_rate : float, default=1.0\n",
      " |      Weight applied to each regressor at each boosting iteration. A higher\n",
      " |      learning rate increases the contribution of each regressor. There is\n",
      " |      a trade-off between the `learning_rate` and `n_estimators` parameters.\n",
      " |      Values must be in the range `(0.0, inf)`.\n",
      " |  \n",
      " |  loss : {'linear', 'square', 'exponential'}, default='linear'\n",
      " |      The loss function to use when updating the weights after each\n",
      " |      boosting iteration.\n",
      " |  \n",
      " |  random_state : int, RandomState instance or None, default=None\n",
      " |      Controls the random seed given at each `estimator` at each\n",
      " |      boosting iteration.\n",
      " |      Thus, it is only used when `estimator` exposes a `random_state`.\n",
      " |      In addition, it controls the bootstrap of the weights used to train the\n",
      " |      `estimator` at each boosting iteration.\n",
      " |      Pass an int for reproducible output across multiple function calls.\n",
      " |      See :term:`Glossary <random_state>`.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  estimator_ : estimator\n",
      " |      The base estimator from which the ensemble is grown.\n",
      " |  \n",
      " |      .. versionadded:: 1.2\n",
      " |         `base_estimator_` was renamed to `estimator_`.\n",
      " |  \n",
      " |  estimators_ : list of regressors\n",
      " |      The collection of fitted sub-estimators.\n",
      " |  \n",
      " |  estimator_weights_ : ndarray of floats\n",
      " |      Weights for each estimator in the boosted ensemble.\n",
      " |  \n",
      " |  estimator_errors_ : ndarray of floats\n",
      " |      Regression error for each estimator in the boosted ensemble.\n",
      " |  \n",
      " |  feature_importances_ : ndarray of shape (n_features,)\n",
      " |      The impurity-based feature importances if supported by the\n",
      " |      ``estimator`` (when based on decision trees).\n",
      " |  \n",
      " |      Warning: impurity-based feature importances can be misleading for\n",
      " |      high cardinality features (many unique values). See\n",
      " |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      " |  \n",
      " |  n_features_in_ : int\n",
      " |      Number of features seen during :term:`fit`.\n",
      " |  \n",
      " |      .. versionadded:: 0.24\n",
      " |  \n",
      " |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      " |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      " |      has feature names that are all strings.\n",
      " |  \n",
      " |      .. versionadded:: 1.0\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  AdaBoostClassifier : An AdaBoost classifier.\n",
      " |  GradientBoostingRegressor : Gradient Boosting Classification Tree.\n",
      " |  sklearn.tree.DecisionTreeRegressor : A decision tree regressor.\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  .. [1] Y. Freund, R. Schapire, \"A Decision-Theoretic Generalization of\n",
      " |         on-Line Learning and an Application to Boosting\", 1995.\n",
      " |  \n",
      " |  .. [2] H. Drucker, \"Improving Regressors using Boosting Techniques\", 1997.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.ensemble import AdaBoostRegressor\n",
      " |  >>> from sklearn.datasets import make_regression\n",
      " |  >>> X, y = make_regression(n_features=4, n_informative=2,\n",
      " |  ...                        random_state=0, shuffle=False)\n",
      " |  >>> regr = AdaBoostRegressor(random_state=0, n_estimators=100)\n",
      " |  >>> regr.fit(X, y)\n",
      " |  AdaBoostRegressor(n_estimators=100, random_state=0)\n",
      " |  >>> regr.predict([[0, 0, 0, 0]])\n",
      " |  array([4.7972...])\n",
      " |  >>> regr.score(X, y)\n",
      " |  0.9771...\n",
      " |  \n",
      " |  For a detailed example of utilizing :class:`~sklearn.ensemble.AdaBoostRegressor`\n",
      " |  to fit a sequence of decision trees as weak learners, please refer to\n",
      " |  :ref:`sphx_glr_auto_examples_ensemble_plot_adaboost_regression.py`.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      AdaBoostRegressor\n",
      " |      sklearn.utils._metadata_requests._RoutingNotSupportedMixin\n",
      " |      sklearn.base.RegressorMixin\n",
      " |      BaseWeightBoosting\n",
      " |      sklearn.ensemble._base.BaseEnsemble\n",
      " |      sklearn.base.MetaEstimatorMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.utils._estimator_html_repr._HTMLDocumentationLinkMixin\n",
      " |      sklearn.utils._metadata_requests._MetadataRequester\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, estimator=None, *, n_estimators=50, learning_rate=1.0, loss='linear', random_state=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict regression value for X.\n",
      " |      \n",
      " |      The predicted regression value of an input sample is computed\n",
      " |      as the weighted median prediction of the regressors in the ensemble.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The training input samples. Sparse matrix can be CSC, CSR, COO,\n",
      " |          DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : ndarray of shape (n_samples,)\n",
      " |          The predicted regression values.\n",
      " |  \n",
      " |  set_fit_request(self: sklearn.ensemble._weight_boosting.AdaBoostRegressor, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.ensemble._weight_boosting.AdaBoostRegressor from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      " |      Request metadata passed to the ``fit`` method.\n",
      " |      \n",
      " |      Note that this method is only relevant if\n",
      " |      ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      " |      Please see :ref:`User Guide <metadata_routing>` on how the routing\n",
      " |      mechanism works.\n",
      " |      \n",
      " |      The options for each parameter are:\n",
      " |      \n",
      " |      - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n",
      " |      \n",
      " |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n",
      " |      \n",
      " |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      " |      \n",
      " |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      " |      \n",
      " |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      " |      existing request. This allows you to change the request for some\n",
      " |      parameters and not others.\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method is only relevant if this estimator is used as a\n",
      " |          sub-estimator of a meta-estimator, e.g. used inside a\n",
      " |          :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      " |          Metadata routing for ``sample_weight`` parameter in ``fit``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          The updated object.\n",
      " |  \n",
      " |  set_score_request(self: sklearn.ensemble._weight_boosting.AdaBoostRegressor, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.ensemble._weight_boosting.AdaBoostRegressor from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      " |      Request metadata passed to the ``score`` method.\n",
      " |      \n",
      " |      Note that this method is only relevant if\n",
      " |      ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      " |      Please see :ref:`User Guide <metadata_routing>` on how the routing\n",
      " |      mechanism works.\n",
      " |      \n",
      " |      The options for each parameter are:\n",
      " |      \n",
      " |      - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n",
      " |      \n",
      " |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n",
      " |      \n",
      " |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      " |      \n",
      " |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      " |      \n",
      " |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      " |      existing request. This allows you to change the request for some\n",
      " |      parameters and not others.\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method is only relevant if this estimator is used as a\n",
      " |          sub-estimator of a meta-estimator, e.g. used inside a\n",
      " |          :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      " |          Metadata routing for ``sample_weight`` parameter in ``score``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          The updated object.\n",
      " |  \n",
      " |  staged_predict(self, X)\n",
      " |      Return staged predictions for X.\n",
      " |      \n",
      " |      The predicted regression value of an input sample is computed\n",
      " |      as the weighted median prediction of the regressors in the ensemble.\n",
      " |      \n",
      " |      This generator method yields the ensemble prediction after each\n",
      " |      iteration of boosting and therefore allows monitoring, such as to\n",
      " |      determine the prediction on a test set after each boost.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The training input samples.\n",
      " |      \n",
      " |      Yields\n",
      " |      ------\n",
      " |      y : generator of ndarray of shape (n_samples,)\n",
      " |          The predicted regression values.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  __annotations__ = {'_parameter_constraints': <class 'dict'>}\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.utils._metadata_requests._RoutingNotSupportedMixin:\n",
      " |  \n",
      " |  get_metadata_routing(self)\n",
      " |      Raise `NotImplementedError`.\n",
      " |      \n",
      " |      This estimator does not support metadata routing yet.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.utils._metadata_requests._RoutingNotSupportedMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.RegressorMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Return the coefficient of determination of the prediction.\n",
      " |      \n",
      " |      The coefficient of determination :math:`R^2` is defined as\n",
      " |      :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual\n",
      " |      sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n",
      " |      is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n",
      " |      The best possible score is 1.0 and it can be negative (because the\n",
      " |      model can be arbitrarily worse). A constant model that always predicts\n",
      " |      the expected value of `y`, disregarding the input features, would get\n",
      " |      a :math:`R^2` score of 0.0.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Test samples. For some estimators this may be a precomputed\n",
      " |          kernel matrix or a list of generic objects instead with shape\n",
      " |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      " |          is the number of samples used in the fitting for the estimator.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          True values for `X`.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          :math:`R^2` of ``self.predict(X)`` w.r.t. `y`.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      " |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      " |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      " |      This influences the ``score`` method of all the multioutput\n",
      " |      regressors (except for\n",
      " |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseWeightBoosting:\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None)\n",
      " |      Build a boosted classifier/regressor from the training set (X, y).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The training input samples. Sparse matrix can be CSC, CSR, COO,\n",
      " |          DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,)\n",
      " |          The target values.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights. If None, the sample weights are initialized to\n",
      " |          1 / n_samples.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Fitted estimator.\n",
      " |  \n",
      " |  staged_score(self, X, y, sample_weight=None)\n",
      " |      Return staged scores for X, y.\n",
      " |      \n",
      " |      This generator method yields the ensemble score after each iteration of\n",
      " |      boosting and therefore allows monitoring, such as to determine the\n",
      " |      score on a test set after each boost.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The training input samples. Sparse matrix can be CSC, CSR, COO,\n",
      " |          DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,)\n",
      " |          Labels for X.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Yields\n",
      " |      ------\n",
      " |      z : float\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from BaseWeightBoosting:\n",
      " |  \n",
      " |  feature_importances_\n",
      " |      The impurity-based feature importances.\n",
      " |      \n",
      " |      The higher, the more important the feature.\n",
      " |      The importance of a feature is computed as the (normalized)\n",
      " |      total reduction of the criterion brought by that feature.  It is also\n",
      " |      known as the Gini importance.\n",
      " |      \n",
      " |      Warning: impurity-based feature importances can be misleading for\n",
      " |      high cardinality features (many unique values). See\n",
      " |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      feature_importances_ : ndarray of shape (n_features,)\n",
      " |          The feature importances.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.ensemble._base.BaseEnsemble:\n",
      " |  \n",
      " |  __getitem__(self, index)\n",
      " |      Return the index'th estimator in the ensemble.\n",
      " |  \n",
      " |  __iter__(self)\n",
      " |      Return iterator over estimators in the ensemble.\n",
      " |  \n",
      " |  __len__(self)\n",
      " |      Return the number of estimators in the ensemble.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |      Helper for pickle.\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  __sklearn_clone__(self)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      " |      possible to update each component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      " |  \n",
      " |  __init_subclass__(**kwargs)\n",
      " |      Set the ``set_{method}_request`` methods.\n",
      " |      \n",
      " |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It\n",
      " |      looks for the information available in the set default values which are\n",
      " |      set using ``__metadata_request__*`` class attributes, or inferred\n",
      " |      from method signatures.\n",
      " |      \n",
      " |      The ``__metadata_request__*`` class attributes are used when a method\n",
      " |      does not explicitly accept a metadata through its arguments or if the\n",
      " |      developer would like to specify a request value for those metadata\n",
      " |      which are different from the default ``None``.\n",
      " |      \n",
      " |      References\n",
      " |      ----------\n",
      " |      .. [1] https://www.python.org/dev/peps/pep-0487\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T13:01:42.891277Z",
     "start_time": "2025-06-15T13:01:42.889022Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sklearn.metrics\n",
    "print(sklearn.metrics.__all__)"
   ],
   "id": "186762d69fa39a6a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['accuracy_score', 'adjusted_mutual_info_score', 'adjusted_rand_score', 'auc', 'average_precision_score', 'balanced_accuracy_score', 'calinski_harabasz_score', 'check_scoring', 'class_likelihood_ratios', 'classification_report', 'cluster', 'cohen_kappa_score', 'completeness_score', 'ConfusionMatrixDisplay', 'confusion_matrix', 'consensus_score', 'coverage_error', 'd2_tweedie_score', 'd2_absolute_error_score', 'd2_log_loss_score', 'd2_pinball_score', 'dcg_score', 'davies_bouldin_score', 'DetCurveDisplay', 'det_curve', 'DistanceMetric', 'euclidean_distances', 'explained_variance_score', 'f1_score', 'fbeta_score', 'fowlkes_mallows_score', 'get_scorer', 'hamming_loss', 'hinge_loss', 'homogeneity_completeness_v_measure', 'homogeneity_score', 'jaccard_score', 'label_ranking_average_precision_score', 'label_ranking_loss', 'log_loss', 'make_scorer', 'nan_euclidean_distances', 'matthews_corrcoef', 'max_error', 'mean_absolute_error', 'mean_squared_error', 'mean_squared_log_error', 'mean_pinball_loss', 'mean_poisson_deviance', 'mean_gamma_deviance', 'mean_tweedie_deviance', 'median_absolute_error', 'mean_absolute_percentage_error', 'multilabel_confusion_matrix', 'mutual_info_score', 'ndcg_score', 'normalized_mutual_info_score', 'pair_confusion_matrix', 'pairwise_distances', 'pairwise_distances_argmin', 'pairwise_distances_argmin_min', 'pairwise_distances_chunked', 'pairwise_kernels', 'PrecisionRecallDisplay', 'precision_recall_curve', 'precision_recall_fscore_support', 'precision_score', 'PredictionErrorDisplay', 'r2_score', 'rand_score', 'recall_score', 'RocCurveDisplay', 'roc_auc_score', 'roc_curve', 'root_mean_squared_log_error', 'root_mean_squared_error', 'get_scorer_names', 'silhouette_samples', 'silhouette_score', 'top_k_accuracy_score', 'v_measure_score', 'zero_one_loss', 'brier_score_loss']\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T12:55:00.750931Z",
     "start_time": "2025-06-15T12:55:00.744424Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from scipy import stats\n",
    "print(help(stats.chi2_contingency))"
   ],
   "id": "b7ff97d6b58762c8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function chi2_contingency in module scipy.stats.contingency:\n",
      "\n",
      "chi2_contingency(observed, correction=True, lambda_=None, *, method=None)\n",
      "    Chi-square test of independence of variables in a contingency table.\n",
      "    \n",
      "    This function computes the chi-square statistic and p-value for the\n",
      "    hypothesis test of independence of the observed frequencies in the\n",
      "    contingency table [1]_ `observed`.  The expected frequencies are computed\n",
      "    based on the marginal sums under the assumption of independence; see\n",
      "    `scipy.stats.contingency.expected_freq`.  The number of degrees of\n",
      "    freedom is (expressed using numpy functions and attributes)::\n",
      "    \n",
      "        dof = observed.size - sum(observed.shape) + observed.ndim - 1\n",
      "    \n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    observed : array_like\n",
      "        The contingency table. The table contains the observed frequencies\n",
      "        (i.e. number of occurrences) in each category.  In the two-dimensional\n",
      "        case, the table is often described as an \"R x C table\".\n",
      "    correction : bool, optional\n",
      "        If True, *and* the degrees of freedom is 1, apply Yates' correction\n",
      "        for continuity.  The effect of the correction is to adjust each\n",
      "        observed value by 0.5 towards the corresponding expected value.\n",
      "    lambda_ : float or str, optional\n",
      "        By default, the statistic computed in this test is Pearson's\n",
      "        chi-squared statistic [2]_.  `lambda_` allows a statistic from the\n",
      "        Cressie-Read power divergence family [3]_ to be used instead.  See\n",
      "        `scipy.stats.power_divergence` for details.\n",
      "    method : ResamplingMethod, optional\n",
      "        Defines the method used to compute the p-value. Compatible only with\n",
      "        `correction=False`,  default `lambda_`, and two-way tables.\n",
      "        If `method` is an instance of `PermutationMethod`/`MonteCarloMethod`,\n",
      "        the p-value is computed using\n",
      "        `scipy.stats.permutation_test`/`scipy.stats.monte_carlo_test` with the\n",
      "        provided configuration options and other appropriate settings.\n",
      "        Otherwise, the p-value is computed as documented in the notes.\n",
      "        Note that if `method` is an instance of `MonteCarloMethod`, the ``rvs``\n",
      "        attribute must be left unspecified; Monte Carlo samples are always drawn\n",
      "        using the ``rvs`` method of `scipy.stats.random_table`.\n",
      "    \n",
      "        .. versionadded:: 1.15.0\n",
      "    \n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    res : Chi2ContingencyResult\n",
      "        An object containing attributes:\n",
      "    \n",
      "        statistic : float\n",
      "            The test statistic.\n",
      "        pvalue : float\n",
      "            The p-value of the test.\n",
      "        dof : int\n",
      "            The degrees of freedom. NaN if `method` is not ``None``.\n",
      "        expected_freq : ndarray, same shape as `observed`\n",
      "            The expected frequencies, based on the marginal sums of the table.\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    scipy.stats.contingency.expected_freq\n",
      "    scipy.stats.fisher_exact\n",
      "    scipy.stats.chisquare\n",
      "    scipy.stats.power_divergence\n",
      "    scipy.stats.barnard_exact\n",
      "    scipy.stats.boschloo_exact\n",
      "    :ref:`hypothesis_chi2_contingency` : Extended example\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    An often quoted guideline for the validity of this calculation is that\n",
      "    the test should be used only if the observed and expected frequencies\n",
      "    in each cell are at least 5.\n",
      "    \n",
      "    This is a test for the independence of different categories of a\n",
      "    population. The test is only meaningful when the dimension of\n",
      "    `observed` is two or more.  Applying the test to a one-dimensional\n",
      "    table will always result in `expected` equal to `observed` and a\n",
      "    chi-square statistic equal to 0.\n",
      "    \n",
      "    This function does not handle masked arrays, because the calculation\n",
      "    does not make sense with missing values.\n",
      "    \n",
      "    Like `scipy.stats.chisquare`, this function computes a chi-square\n",
      "    statistic; the convenience this function provides is to figure out the\n",
      "    expected frequencies and degrees of freedom from the given contingency\n",
      "    table. If these were already known, and if the Yates' correction was not\n",
      "    required, one could use `scipy.stats.chisquare`.  That is, if one calls::\n",
      "    \n",
      "        res = chi2_contingency(obs, correction=False)\n",
      "    \n",
      "    then the following is true::\n",
      "    \n",
      "        (res.statistic, res.pvalue) == stats.chisquare(obs.ravel(),\n",
      "                                                       f_exp=ex.ravel(),\n",
      "                                                       ddof=obs.size - 1 - dof)\n",
      "    \n",
      "    The `lambda_` argument was added in version 0.13.0 of scipy.\n",
      "    \n",
      "    References\n",
      "    ----------\n",
      "    .. [1] \"Contingency table\",\n",
      "           https://en.wikipedia.org/wiki/Contingency_table\n",
      "    .. [2] \"Pearson's chi-squared test\",\n",
      "           https://en.wikipedia.org/wiki/Pearson%27s_chi-squared_test\n",
      "    .. [3] Cressie, N. and Read, T. R. C., \"Multinomial Goodness-of-Fit\n",
      "           Tests\", J. Royal Stat. Soc. Series B, Vol. 46, No. 3 (1984),\n",
      "           pp. 440-464.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    A two-way example (2 x 3):\n",
      "    \n",
      "    >>> import numpy as np\n",
      "    >>> from scipy.stats import chi2_contingency\n",
      "    >>> obs = np.array([[10, 10, 20], [20, 20, 20]])\n",
      "    >>> res = chi2_contingency(obs)\n",
      "    >>> res.statistic\n",
      "    2.7777777777777777\n",
      "    >>> res.pvalue\n",
      "    0.24935220877729619\n",
      "    >>> res.dof\n",
      "    2\n",
      "    >>> res.expected_freq\n",
      "    array([[ 12.,  12.,  16.],\n",
      "           [ 18.,  18.,  24.]])\n",
      "    \n",
      "    Perform the test using the log-likelihood ratio (i.e. the \"G-test\")\n",
      "    instead of Pearson's chi-squared statistic.\n",
      "    \n",
      "    >>> res = chi2_contingency(obs, lambda_=\"log-likelihood\")\n",
      "    >>> res.statistic\n",
      "    2.7688587616781319\n",
      "    >>> res.pvalue\n",
      "    0.25046668010954165\n",
      "    \n",
      "    A four-way example (2 x 2 x 2 x 2):\n",
      "    \n",
      "    >>> obs = np.array(\n",
      "    ...     [[[[12, 17],\n",
      "    ...        [11, 16]],\n",
      "    ...       [[11, 12],\n",
      "    ...        [15, 16]]],\n",
      "    ...      [[[23, 15],\n",
      "    ...        [30, 22]],\n",
      "    ...       [[14, 17],\n",
      "    ...        [15, 16]]]])\n",
      "    >>> res = chi2_contingency(obs)\n",
      "    >>> res.statistic\n",
      "    8.7584514426741897\n",
      "    >>> res.pvalue\n",
      "    0.64417725029295503\n",
      "    \n",
      "    When the sum of the elements in a two-way table is small, the p-value\n",
      "    produced by the default asymptotic approximation may be inaccurate.\n",
      "    Consider passing a `PermutationMethod` or `MonteCarloMethod` as the\n",
      "    `method` parameter with `correction=False`.\n",
      "    \n",
      "    >>> from scipy.stats import PermutationMethod\n",
      "    >>> obs = np.asarray([[12, 3],\n",
      "    ...                   [17, 16]])\n",
      "    >>> res = chi2_contingency(obs, correction=False)\n",
      "    >>> ref = chi2_contingency(obs, correction=False, method=PermutationMethod())\n",
      "    >>> res.pvalue, ref.pvalue\n",
      "    (0.0614122539870913, 0.1074)  # may vary\n",
      "    \n",
      "    For a more detailed example, see :ref:`hypothesis_chi2_contingency`.\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T12:55:35.721865Z",
     "start_time": "2025-06-15T12:55:35.719603Z"
    }
   },
   "cell_type": "code",
   "source": "print(stats.__all__)",
   "id": "5c6bca7652975025",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BootstrapMethod', 'CensoredData', 'ConstantInputWarning', 'Covariance', 'DegenerateDataWarning', 'FitError', 'Mixture', 'MonteCarloMethod', 'NearConstantInputWarning', 'Normal', 'PermutationMethod', 'Uniform', 'abs', 'alexandergovern', 'alpha', 'anderson', 'anderson_ksamp', 'anglit', 'ansari', 'arcsine', 'argus', 'barnard_exact', 'bartlett', 'bayes_mvs', 'bernoulli', 'beta', 'betabinom', 'betanbinom', 'betaprime', 'biasedurn', 'binned_statistic', 'binned_statistic_2d', 'binned_statistic_dd', 'binom', 'binomtest', 'boltzmann', 'bootstrap', 'boschloo_exact', 'boxcox', 'boxcox_llf', 'boxcox_normmax', 'boxcox_normplot', 'bradford', 'brunnermunzel', 'burr', 'burr12', 'bws_test', 'cauchy', 'chatterjeexi', 'chi', 'chi2', 'chi2_contingency', 'chisquare', 'circmean', 'circstd', 'circvar', 'combine_pvalues', 'contingency', 'cosine', 'cramervonmises', 'cramervonmises_2samp', 'crystalball', 'cumfreq', 'describe', 'dgamma', 'differential_entropy', 'directional_stats', 'dirichlet', 'dirichlet_multinomial', 'distributions', 'dlaplace', 'dpareto_lognorm', 'dunnett', 'dweibull', 'ecdf', 'energy_distance', 'entropy', 'epps_singleton_2samp', 'erlang', 'exp', 'expectile', 'expon', 'exponnorm', 'exponpow', 'exponweib', 'f', 'f_oneway', 'false_discovery_control', 'fatiguelife', 'find_repeats', 'fisher_exact', 'fisk', 'fit', 'fligner', 'foldcauchy', 'foldnorm', 'friedmanchisquare', 'gamma', 'gausshyper', 'gaussian_kde', 'genexpon', 'genextreme', 'gengamma', 'genhalflogistic', 'genhyperbolic', 'geninvgauss', 'genlogistic', 'gennorm', 'genpareto', 'geom', 'gibrat', 'gmean', 'gompertz', 'goodness_of_fit', 'gstd', 'gumbel_l', 'gumbel_r', 'gzscore', 'halfcauchy', 'halfgennorm', 'halflogistic', 'halfnorm', 'hmean', 'hypergeom', 'hypsecant', 'invgamma', 'invgauss', 'invweibull', 'invwishart', 'iqr', 'irwinhall', 'jarque_bera', 'jf_skew_t', 'johnsonsb', 'johnsonsu', 'kappa3', 'kappa4', 'kde', 'kendalltau', 'kruskal', 'ks_1samp', 'ks_2samp', 'ksone', 'kstat', 'kstatvar', 'kstest', 'kstwo', 'kstwobign', 'kurtosis', 'kurtosistest', 'landau', 'laplace', 'laplace_asymmetric', 'levene', 'levy', 'levy_l', 'levy_stable', 'linregress', 'lmoment', 'log', 'loggamma', 'logistic', 'loglaplace', 'lognorm', 'logrank', 'logser', 'loguniform', 'lomax', 'make_distribution', 'mannwhitneyu', 'matrix_normal', 'maxwell', 'median_abs_deviation', 'median_test', 'mielke', 'mode', 'moment', 'monte_carlo_test', 'mood', 'morestats', 'moyal', 'mstats', 'mstats_basic', 'mstats_extras', 'multinomial', 'multiscale_graphcorr', 'multivariate_hypergeom', 'multivariate_normal', 'multivariate_t', 'mvn', 'mvsdist', 'nakagami', 'nbinom', 'ncf', 'nchypergeom_fisher', 'nchypergeom_wallenius', 'nct', 'ncx2', 'nhypergeom', 'norm', 'normal_inverse_gamma', 'normaltest', 'norminvgauss', 'obrientransform', 'order_statistic', 'ortho_group', 'page_trend_test', 'pareto', 'pearson3', 'pearsonr', 'percentileofscore', 'permutation_test', 'planck', 'pmean', 'pointbiserialr', 'poisson', 'poisson_binom', 'poisson_means_test', 'power', 'power_divergence', 'powerlaw', 'powerlognorm', 'powernorm', 'ppcc_max', 'ppcc_plot', 'probplot', 'qmc', 'quantile_test', 'randint', 'random_correlation', 'random_table', 'rankdata', 'ranksums', 'rayleigh', 'rdist', 'recipinvgauss', 'reciprocal', 'rel_breitwigner', 'relfreq', 'rice', 'rv_continuous', 'rv_discrete', 'rv_histogram', 'scoreatpercentile', 'sem', 'semicircular', 'shapiro', 'siegelslopes', 'sigmaclip', 'skellam', 'skew', 'skewcauchy', 'skewnorm', 'skewtest', 'sobol_indices', 'somersd', 'spearmanr', 'special_ortho_group', 'stats', 'studentized_range', 't', 'theilslopes', 'tiecorrect', 'tmax', 'tmean', 'tmin', 'trapezoid', 'trapz', 'triang', 'trim1', 'trim_mean', 'trimboth', 'truncate', 'truncexpon', 'truncnorm', 'truncpareto', 'truncweibull_min', 'tsem', 'tstd', 'ttest_1samp', 'ttest_ind', 'ttest_ind_from_stats', 'ttest_rel', 'tukey_hsd', 'tukeylambda', 'tvar', 'uniform', 'uniform_direction', 'unitary_group', 'variation', 'vonmises', 'vonmises_fisher', 'vonmises_line', 'wald', 'wasserstein_distance', 'wasserstein_distance_nd', 'weibull_max', 'weibull_min', 'weightedtau', 'wilcoxon', 'wishart', 'wrapcauchy', 'yeojohnson', 'yeojohnson_llf', 'yeojohnson_normmax', 'yeojohnson_normplot', 'yulesimon', 'zipf', 'zipfian', 'zmap', 'zscore']\n"
     ]
    }
   ],
   "execution_count": 16
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
